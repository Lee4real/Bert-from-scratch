{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Custom Question-Answering Model with PyTorch and SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:49:34.405941Z",
     "iopub.status.busy": "2024-11-16T22:49:34.405652Z",
     "iopub.status.idle": "2024-11-16T22:49:40.588106Z",
     "shell.execute_reply": "2024-11-16T22:49:40.587107Z",
     "shell.execute_reply.started": "2024-11-16T22:49:34.405909Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:49:53.628636Z",
     "iopub.status.busy": "2024-11-16T22:49:53.628256Z",
     "iopub.status.idle": "2024-11-16T22:49:53.635180Z",
     "shell.execute_reply": "2024-11-16T22:49:53.633845Z",
     "shell.execute_reply.started": "2024-11-16T22:49:53.628600Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "# Device configuration - enable multiple GPUs if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_count = torch.cuda.device_count()\n",
    "print(f\"Using {device_count} GPUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:49:55.064385Z",
     "iopub.status.busy": "2024-11-16T22:49:55.064034Z",
     "iopub.status.idle": "2024-11-16T22:49:59.969077Z",
     "shell.execute_reply": "2024-11-16T22:49:59.968202Z",
     "shell.execute_reply.started": "2024-11-16T22:49:55.064350Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d54db6fcb26477a916630f2c9ae4237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f3bbe4016ee4c1ca6a7eb03235ec632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68b0a4991c74266850d0fcb9abd431e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81755f10e4f14ee88d3c444c8394e485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df847eb1dcec45bcad0cac0236e49aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the SQuAD dataset\n",
    "dataset = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:50:04.709604Z",
     "iopub.status.busy": "2024-11-16T22:50:04.709239Z",
     "iopub.status.idle": "2024-11-16T22:50:04.716207Z",
     "shell.execute_reply": "2024-11-16T22:50:04.715319Z",
     "shell.execute_reply.started": "2024-11-16T22:50:04.709568Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:50:07.198604Z",
     "iopub.status.busy": "2024-11-16T22:50:07.198223Z",
     "iopub.status.idle": "2024-11-16T22:50:08.943694Z",
     "shell.execute_reply": "2024-11-16T22:50:08.942669Z",
     "shell.execute_reply.started": "2024-11-16T22:50:07.198567Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec773daca7b40bcbe317bcc9337ac6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b82d24a0023444fb1c90dc17acb4c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069605b768ec47e1b95aa54f0623b640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93542fd43d8343cf9d772e357ee4e848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load BERT tokenizer\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Load the fast tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:50:11.833073Z",
     "iopub.status.busy": "2024-11-16T22:50:11.832670Z",
     "iopub.status.idle": "2024-11-16T22:50:11.838609Z",
     "shell.execute_reply": "2024-11-16T22:50:11.837533Z",
     "shell.execute_reply.started": "2024-11-16T22:50:11.833029Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:50:14.016454Z",
     "iopub.status.busy": "2024-11-16T22:50:14.015539Z",
     "iopub.status.idle": "2024-11-16T22:50:14.028177Z",
     "shell.execute_reply": "2024-11-16T22:50:14.027143Z",
     "shell.execute_reply.started": "2024-11-16T22:50:14.016396Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Tokenize the inputs\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",  # Truncate the context if it's too long\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        return_offsets_mapping=True,  # Get the mapping between tokens and original text\n",
    "    )\n",
    "\n",
    "    # Process start and end positions\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offsets in enumerate(tokenized[\"offset_mapping\"]):\n",
    "        answer = examples[\"answers\"][i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer[\"text\"][0])\n",
    "\n",
    "        # Find the token indices corresponding to the answer span\n",
    "        sequence_ids = tokenized.sequence_ids(i)\n",
    "        context_start = sequence_ids.index(1)\n",
    "        context_end = len(sequence_ids) - sequence_ids[::-1].index(1)\n",
    "\n",
    "        token_start = context_start\n",
    "        token_end = context_end - 1\n",
    "\n",
    "        for idx, (start, end) in enumerate(offsets):\n",
    "            if start <= start_char < end:\n",
    "                token_start = idx\n",
    "            if start < end_char <= end:\n",
    "                token_end = idx\n",
    "                break\n",
    "\n",
    "        # Set to CLS token index if the answer is not fully inside context\n",
    "        if token_start < context_start or token_end >= context_end:\n",
    "            token_start = token_end = tokenizer.cls_token_id\n",
    "\n",
    "        start_positions.append(token_start)\n",
    "        end_positions.append(token_end)\n",
    "\n",
    "    # Add positions to the tokenized output\n",
    "    tokenized[\"start_positions\"] = start_positions\n",
    "    tokenized[\"end_positions\"] = end_positions\n",
    "\n",
    "    # Remove unnecessary fields\n",
    "    tokenized.pop(\"offset_mapping\", None)\n",
    "\n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:50:16.196811Z",
     "iopub.status.busy": "2024-11-16T22:50:16.196433Z",
     "iopub.status.idle": "2024-11-16T22:51:25.526769Z",
     "shell.execute_reply": "2024-11-16T22:51:25.525919Z",
     "shell.execute_reply.started": "2024-11-16T22:50:16.196767Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d16d459f79d447febcdac027cbf40ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2f549a61d44522a56ed9a9dd0db4f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:51:25.561750Z",
     "iopub.status.busy": "2024-11-16T22:51:25.561475Z",
     "iopub.status.idle": "2024-11-16T22:51:25.622345Z",
     "shell.execute_reply": "2024-11-16T22:51:25.621549Z",
     "shell.execute_reply.started": "2024-11-16T22:51:25.561720Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SquadDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for the SQuAD data.\"\"\"\n",
    "    def __init__(self, encodings):\n",
    "        # Convert the Hugging Face dataset columns to torch tensors\n",
    "        self.encodings = {key: torch.tensor(value) for key, value in encodings.items()}  # Ensure the values are tensors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: tensor[idx] for key, tensor in self.encodings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:51:54.872862Z",
     "iopub.status.busy": "2024-11-16T22:51:54.872478Z",
     "iopub.status.idle": "2024-11-16T22:52:10.466722Z",
     "shell.execute_reply": "2024-11-16T22:52:10.465907Z",
     "shell.execute_reply.started": "2024-11-16T22:51:54.872825Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert dataset to PyTorch Dataset\n",
    "train_dataset = SquadDataset(tokenized_datasets[\"train\"][:10000])\n",
    "valid_dataset = SquadDataset(tokenized_datasets[\"validation\"][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:52:10.468580Z",
     "iopub.status.busy": "2024-11-16T22:52:10.468263Z",
     "iopub.status.idle": "2024-11-16T22:52:10.509392Z",
     "shell.execute_reply": "2024-11-16T22:52:10.508563Z",
     "shell.execute_reply.started": "2024-11-16T22:52:10.468547Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  2000,  3183,  2106,  1996,  6261,  2984,  9382,  3711,  1999,\n",
       "          8517,  1999, 10223, 26371,  2605,  1029,   102,  6549,  2135,  1010,\n",
       "          1996,  2082,  2038,  1037,  3234,  2839,  1012, 10234,  1996,  2364,\n",
       "          2311,  1005,  1055,  2751,  8514,  2003,  1037,  3585,  6231,  1997,\n",
       "          1996,  6261,  2984,  1012,  3202,  1999,  2392,  1997,  1996,  2364,\n",
       "          2311,  1998,  5307,  2009,  1010,  2003,  1037,  6967,  6231,  1997,\n",
       "          4828,  2007,  2608,  2039, 14995,  6924,  2007,  1996,  5722,  1000,\n",
       "          2310,  3490,  2618,  4748,  2033, 18168,  5267,  1000,  1012,  2279,\n",
       "          2000,  1996,  2364,  2311,  2003,  1996, 13546,  1997,  1996,  6730,\n",
       "          2540,  1012,  3202,  2369,  1996, 13546,  2003,  1996, 24665, 23052,\n",
       "          1010,  1037, 14042,  2173,  1997,  7083,  1998,  9185,  1012,  2009,\n",
       "          2003,  1037, 15059,  1997,  1996, 24665, 23052,  2012, 10223, 26371,\n",
       "          1010,  2605,  2073,  1996,  6261,  2984, 22353,  2135,  2596,  2000,\n",
       "          3002, 16595,  9648,  4674,  2061, 12083,  9711,  2271,  1999,  8517,\n",
       "          1012,  2012,  1996,  2203,  1997,  1996,  2364,  3298,  1006,  1998,\n",
       "          1999,  1037,  3622,  2240,  2008,  8539,  2083,  1017, 11342,  1998,\n",
       "          1996,  2751,  8514,  1007,  1010,  2003,  1037,  3722,  1010,  2715,\n",
       "          2962,  6231,  1997,  2984,  1012,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'start_positions': tensor(130),\n",
       " 'end_positions': tensor(137)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:52:10.510639Z",
     "iopub.status.busy": "2024-11-16T22:52:10.510382Z",
     "iopub.status.idle": "2024-11-16T22:52:10.515958Z",
     "shell.execute_reply": "2024-11-16T22:52:10.515041Z",
     "shell.execute_reply.started": "2024-11-16T22:52:10.510609Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:52:10.518465Z",
     "iopub.status.busy": "2024-11-16T22:52:10.517933Z",
     "iopub.status.idle": "2024-11-16T22:52:10.527619Z",
     "shell.execute_reply": "2024-11-16T22:52:10.526747Z",
     "shell.execute_reply.started": "2024-11-16T22:52:10.518420Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:52:34.021433Z",
     "iopub.status.busy": "2024-11-16T22:52:34.020817Z",
     "iopub.status.idle": "2024-11-16T22:52:34.029969Z",
     "shell.execute_reply": "2024-11-16T22:52:34.029003Z",
     "shell.execute_reply.started": "2024-11-16T22:52:34.021394Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BERTEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, max_position_embeddings):\n",
    "        super(BERTEmbeddings, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(2, hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, position_ids=None):\n",
    "        seq_length = input_ids.size(1)\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(seq_length, device=input_ids.device).unsqueeze(0).expand(input_ids.size(0), -1)\n",
    "        word_embeddings = self.word_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids if token_type_ids is not None else torch.zeros_like(input_ids))\n",
    "        embeddings = word_embeddings + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:52:34.943298Z",
     "iopub.status.busy": "2024-11-16T22:52:34.942906Z",
     "iopub.status.idle": "2024-11-16T22:52:34.953795Z",
     "shell.execute_reply": "2024-11-16T22:52:34.952819Z",
     "shell.execute_reply.started": "2024-11-16T22:52:34.943262Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.attention_head_size = hidden_size // num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        self.query = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        return x.view(*new_shape).permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
    "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2)) / (self.attention_head_size ** 0.5)\n",
    "        attention_probs = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        return self.dense(context_layer.view(*new_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:52:35.659642Z",
     "iopub.status.busy": "2024-11-16T22:52:35.659292Z",
     "iopub.status.idle": "2024-11-16T22:52:35.667388Z",
     "shell.execute_reply": "2024-11-16T22:52:35.666496Z",
     "shell.execute_reply.started": "2024-11-16T22:52:35.659608Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads, intermediate_size):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        self.attention = SelfAttention(hidden_size, num_attention_heads)\n",
    "        self.attention_output = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        self.intermediate = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.output = nn.Sequential(\n",
    "            nn.LayerNorm(intermediate_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(intermediate_size, hidden_size),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        attention_output = self.attention(hidden_states)\n",
    "        hidden_states = self.attention_output(attention_output + hidden_states)\n",
    "        intermediate_output = self.intermediate(hidden_states)\n",
    "        \n",
    "        # Project intermediate_output to match hidden_states' size\n",
    "        hidden_states = self.output(intermediate_output) + hidden_states  # Ensure same dimensions for addition\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:52:36.170833Z",
     "iopub.status.busy": "2024-11-16T22:52:36.170252Z",
     "iopub.status.idle": "2024-11-16T22:52:36.177504Z",
     "shell.execute_reply": "2024-11-16T22:52:36.176620Z",
     "shell.execute_reply.started": "2024-11-16T22:52:36.170797Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BERTModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_attention_heads, num_encoder_layers, intermediate_size, max_position_embeddings):\n",
    "        super(BERTModel, self).__init__()\n",
    "        self.embeddings = BERTEmbeddings(vocab_size, hidden_size, max_position_embeddings)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerLayer(hidden_size, num_attention_heads, intermediate_size) for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, position_ids=None):\n",
    "        hidden_states = self.embeddings(input_ids, token_type_ids, position_ids)\n",
    "        for layer in self.encoder_layers:\n",
    "            hidden_states = layer(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:52:36.650918Z",
     "iopub.status.busy": "2024-11-16T22:52:36.650594Z",
     "iopub.status.idle": "2024-11-16T22:52:36.657421Z",
     "shell.execute_reply": "2024-11-16T22:52:36.656502Z",
     "shell.execute_reply.started": "2024-11-16T22:52:36.650884Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BERTForQA(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(BERTForQA, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.start_logits = nn.Linear(bert_model.embeddings.word_embeddings.embedding_dim, 1)\n",
    "        self.end_logits = nn.Linear(bert_model.embeddings.word_embeddings.embedding_dim, 1)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, position_ids=None):\n",
    "        hidden_states = self.bert(input_ids, token_type_ids, position_ids)\n",
    "        # print(f\"Shape of hidden states: {hidden_states.shape}\")  # Debugging line\n",
    "        start_logits = self.start_logits(hidden_states).squeeze(-1)\n",
    "        end_logits = self.end_logits(hidden_states).squeeze(-1)\n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:52:37.317505Z",
     "iopub.status.busy": "2024-11-16T22:52:37.316680Z",
     "iopub.status.idle": "2024-11-16T22:52:38.488166Z",
     "shell.execute_reply": "2024-11-16T22:52:38.487362Z",
     "shell.execute_reply.started": "2024-11-16T22:52:37.317466Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model = BERTForQA(\n",
    "    BERTModel(vocab_size=30522, hidden_size=768, num_attention_heads=12, num_encoder_layers=12, intermediate_size=3072, max_position_embeddings=512)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:52:41.648863Z",
     "iopub.status.busy": "2024-11-16T22:52:41.647917Z",
     "iopub.status.idle": "2024-11-16T22:52:41.653143Z",
     "shell.execute_reply": "2024-11-16T22:52:41.652265Z",
     "shell.execute_reply.started": "2024-11-16T22:52:41.648808Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if device_count > 1:\n",
    "    model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:52:43.366644Z",
     "iopub.status.busy": "2024-11-16T22:52:43.365735Z",
     "iopub.status.idle": "2024-11-16T22:52:43.372461Z",
     "shell.execute_reply": "2024-11-16T22:52:43.371500Z",
     "shell.execute_reply.started": "2024-11-16T22:52:43.366601Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): BERTForQA(\n",
      "    (bert): BERTModel(\n",
      "      (embeddings): BERTEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder_layers): ModuleList(\n",
      "        (0-11): 12 x TransformerLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (attention_output): Sequential(\n",
      "            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (intermediate): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (output): Sequential(\n",
      "            (0): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (3): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (start_logits): Linear(in_features=768, out_features=1, bias=True)\n",
      "    (end_logits): Linear(in_features=768, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:52:45.222854Z",
     "iopub.status.busy": "2024-11-16T22:52:45.221790Z",
     "iopub.status.idle": "2024-11-16T22:52:46.645951Z",
     "shell.execute_reply": "2024-11-16T22:52:46.645198Z",
     "shell.execute_reply.started": "2024-11-16T22:52:45.222811Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:52:48.420736Z",
     "iopub.status.busy": "2024-11-16T22:52:48.419691Z",
     "iopub.status.idle": "2024-11-16T22:52:48.424832Z",
     "shell.execute_reply": "2024-11-16T22:52:48.423828Z",
     "shell.execute_reply.started": "2024-11-16T22:52:48.420696Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:52:51.392282Z",
     "iopub.status.busy": "2024-11-16T22:52:51.391897Z",
     "iopub.status.idle": "2024-11-16T22:52:51.396414Z",
     "shell.execute_reply": "2024-11-16T22:52:51.395533Z",
     "shell.execute_reply.started": "2024-11-16T22:52:51.392246Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T23:35:12.154081Z",
     "iopub.status.busy": "2024-11-16T23:35:12.153665Z",
     "iopub.status.idle": "2024-11-17T00:02:26.145120Z",
     "shell.execute_reply": "2024-11-17T00:02:26.144173Z",
     "shell.execute_reply.started": "2024-11-16T23:35:12.154042Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 3.9257\n",
      "Epoch 2: Loss = 4.7504\n",
      "Epoch 3: Loss = 3.9344\n",
      "Epoch 4: Loss = 6.2700\n",
      "Epoch 5: Loss = 6.3367\n"
     ]
    }
   ],
   "source": [
    "# Training loop with AMP and device/stream management\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        start_positions = batch[\"start_positions\"].to(device)\n",
    "        end_positions = batch[\"end_positions\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # With autocast and stream management for mixed precision\n",
    "        with autocast('cuda'):   # Mixed precision with AMP\n",
    "            start_logits, end_logits = model(input_ids, token_type_ids)\n",
    "            start_loss = loss_fn(start_logits, start_positions)\n",
    "            end_loss = loss_fn(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "\n",
    "        # Scale the loss and backpropagate\n",
    "        scaler.scale(total_loss).backward()\n",
    "\n",
    "        # Step the optimizer\n",
    "        scaler.step(optimizer)\n",
    "\n",
    "        # Update the scaler for the next step\n",
    "        scaler.update()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}: Loss = {total_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T00:03:08.381431Z",
     "iopub.status.busy": "2024-11-17T00:03:08.381054Z",
     "iopub.status.idle": "2024-11-17T00:03:08.388884Z",
     "shell.execute_reply": "2024-11-17T00:03:08.387862Z",
     "shell.execute_reply.started": "2024-11-17T00:03:08.381396Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict_answer(model, question, context, tokenizer, device):\n",
    "    # Tokenize the input question and context pair\n",
    "    inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "    # Move the input tensors to the correct device (GPU or CPU)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    token_type_ids = inputs[\"token_type_ids\"].to(device)\n",
    "\n",
    "    # Get predictions from the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        start_logits, end_logits = model(input_ids, token_type_ids)\n",
    "\n",
    "    # Get the predicted start and end token indices\n",
    "    start_pred = torch.argmax(start_logits, dim=-1)\n",
    "    end_pred = torch.argmax(end_logits, dim=-1)\n",
    "\n",
    "    # Convert token indices to words (answer span)\n",
    "    answer_tokens = inputs[\"input_ids\"][0][start_pred:end_pred+1]\n",
    "    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    return answer# Sample question and context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T00:03:09.250378Z",
     "iopub.status.busy": "2024-11-17T00:03:09.250055Z",
     "iopub.status.idle": "2024-11-17T00:03:09.279722Z",
     "shell.execute_reply": "2024-11-17T00:03:09.278854Z",
     "shell.execute_reply.started": "2024-11-17T00:03:09.250344Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Answer: paris\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the capital of France?\"\n",
    "context = \"France, located in Western Europe, has Paris as its capital and largest city.\"\n",
    "\n",
    "answer = predict_answer(model, question, context, tokenizer, device)\n",
    "\n",
    "print(f\"Predicted Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T00:03:11.328913Z",
     "iopub.status.busy": "2024-11-17T00:03:11.328187Z",
     "iopub.status.idle": "2024-11-17T00:03:11.358570Z",
     "shell.execute_reply": "2024-11-17T00:03:11.357771Z",
     "shell.execute_reply.started": "2024-11-17T00:03:11.328874Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Answer: george\n"
     ]
    }
   ],
   "source": [
    "question = \"Who wrote the novel '1984'?\"\n",
    "context = \"'1984' is a dystopian social science fiction novel and cautionary tale, written by the English writer George Orwell in 1949.\"\n",
    "\n",
    "answer = predict_answer(model, question, context, tokenizer, device)\n",
    "\n",
    "print(f\"Predicted Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T00:10:46.028800Z",
     "iopub.status.busy": "2024-11-17T00:10:46.028106Z",
     "iopub.status.idle": "2024-11-17T00:10:46.060643Z",
     "shell.execute_reply": "2024-11-17T00:10:46.059878Z",
     "shell.execute_reply.started": "2024-11-17T00:10:46.028758Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Answer: jane\n"
     ]
    }
   ],
   "source": [
    "question = \"Who wrote the novel 'Pride and Prejudice'?\"\n",
    "context = \"'Pride and Prejudice' is a romantic novel of manners written by Jane Austen in 1813. It explores the emotional development of the protagonist, Elizabeth Bennet, who learns the error of making hasty judgments.\"\n",
    "\n",
    "answer = predict_answer(model, question, context, tokenizer, device)\n",
    "\n",
    "print(f\"Predicted Answer: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
